{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking, Ensembling and Voting Techniques\n",
    "\n",
    "In order to improve our score in the competition, in this notebook we will develop advanced techniques based on stacking, ensembling and voting algorithms that will combine multiple learners to boost our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167559, 1183)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sequence = pd.read_hdf('../data/preprocessed/data_sequence_alldata.hdf', key='final_alldata', mode='r')\n",
    "data_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/preprocessed/hashs_train.pkl', 'rb') as fp:\n",
    "    hashs_train = pickle.load(fp)\n",
    "    \n",
    "with open('../data/preprocessed/hashs_test.pkl', 'rb') as fp:\n",
    "    hashs_test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_sequence[data_sequence.hash.isin(hashs_train)]\n",
    "test  = data_sequence[data_sequence.hash.isin(hashs_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_reference = 5\n",
    "\n",
    "with open('../data/preprocessed/original_cols_sequence.pkl', 'rb') as fp: \n",
    "    original_cols = pickle.load(fp)\n",
    "    \n",
    "drop_cols = list(x for x in original_cols if 'exit' in x)\n",
    "drop_cols += ['lat_lon_entry', 'lat_lon_exit']\n",
    "drop_cols += ['euclidean_distance', 'manhattan_distance', 'harvesine_distance',\n",
    "              'center_permanency', 'crossed_city', 'velocity', 'leaving_city', 'entering_city']\n",
    "\n",
    "drop_cols = [col+'_'+str(window_reference) for col in drop_cols]\n",
    "drop_cols += [col for col in train.columns if col.endswith('exit_5')]\n",
    "drop_cols += ['hash', f'delta_last_center_permanency_{window_reference}', f'delta_origin_center_permanency_{window_reference}']\n",
    "\n",
    "features = list(set(train.columns) - set(drop_cols))\n",
    "target   = ['is_inside_city_exit_'+str(window_reference)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100513, 1183), (33505, 1183))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_train, train_val = train_test_split(train[train.hour_exit_5>=15], test_size=0.25, random_state=423)\n",
    "train_train.shape, train_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "model    = LogisticRegression(random_state=20, n_jobs=-1, penalty='l1', C=0.001)\n",
    "imputer  = SimpleImputer(strategy='constant', fill_value=0)\n",
    "lr_pipe  = Pipeline(steps=[('imputer', imputer), ('scaler', MinMaxScaler()), ('model', model)])\n",
    "\n",
    "# LightGBM\n",
    "\n",
    "lgbm_lowdepth = LGBMClassifier(**{'boosting_type': 'gbdt', 'colsample_bytree': 1.0, 'is_unbalance': False, \n",
    "                 'max_depth': 7,  'n_estimators': 150, 'num_leaves': 31, \n",
    "                 'objective': 'binary', 'random_state': 20, 'reg_alpha': 1, 'subsample': 0.7})\n",
    "\n",
    "lgbm_highdepth = LGBMClassifier(n_estimators=100, max_depth=-1)\n",
    "\n",
    "# Random Forest with low depth\n",
    "rf_lowdepth     = RandomForestClassifier(max_depth=7, max_features='sqrt', n_estimators=200, random_state=20)\n",
    "imputer  = SimpleImputer(strategy='constant', fill_value=0)\n",
    "rf_pipe_lowdepth  = Pipeline(steps=[('imputer', imputer), ('model', rf_lowdepth)])\n",
    "\n",
    "# Random Forest with high depth\n",
    "rf_highdepth     = RandomForestClassifier(max_depth=None, max_features=0.75, n_estimators=200, n_jobs=-1, random_state=20)\n",
    "imputer  = SimpleImputer(strategy='constant', fill_value=0)\n",
    "rf_pipe_highdepth  = Pipeline(steps=[('imputer', imputer), ('model', rf_highdepth)])\n",
    "\n",
    "# CatBoost\n",
    "cbc = CatBoostClassifier(verbose=0)\n",
    "\n",
    "# KNN\n",
    "knn      = KNeighborsClassifier(n_neighbors=7, n_jobs=-1)\n",
    "imputer  = SimpleImputer(strategy='constant', fill_value=0)\n",
    "knn_pipe = Pipeline(steps=[('imputer', imputer), ('model', knn)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Hold Out Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100513, 1183), (33505, 1183))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_train, train_val = train_test_split(train[train.hour_exit_5>=15], test_size=0.25, random_state=20)\n",
    "train_train.shape, train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((81414, 1183), (9047, 1183), (10052, 1183), (33505, 1183))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ab, train_c = train_test_split(train_train, test_size=0.1, random_state=20)\n",
    "train_a, train_b  = train_test_split(train_ab, test_size=0.1, random_state=20)\n",
    "\n",
    "train_a.shape, train_b.shape, train_c.shape, train_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting N diverse models on partA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe.fit(train_a[features].values, train_a[target])\n",
    "\n",
    "lr_preds_b = lr_pipe.predict(train_b[features].values)\n",
    "lr_preds_c = lr_pipe.predict(train_c[features].values)\n",
    "lr_preds_val = lr_pipe.predict(train_val[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_lowdepth.fit(train_a[features].values, train_a[target])\n",
    "\n",
    "lgbm_lowdepth_b = lgbm_lowdepth.predict(train_b[features].values)\n",
    "lgbm_lowdepth_c = lgbm_lowdepth.predict(train_c[features].values)\n",
    "lgbm_lowdepth_val = lgbm_lowdepth.predict(train_val[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_highdepth.fit(train_a[features].values, train_a[target])\n",
    "\n",
    "lgbm_highdepth_b = lgbm_highdepth.predict(train_b[features].values)\n",
    "lgbm_highdepth_c = lgbm_highdepth.predict(train_c[features].values)\n",
    "lgbm_highdepth_val = lgbm_highdepth.predict(train_val[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe_lowdepth.fit(train_a[features].values, train_a[target])\n",
    "\n",
    "rf_pipe_lowdepth_b = rf_pipe_lowdepth.predict(train_b[features].values)\n",
    "rf_pipe_lowdepth_c = rf_pipe_lowdepth.predict(train_c[features].values)\n",
    "rf_pipe_lowdepth_val = rf_pipe_lowdepth.predict(train_val[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe_highdepth.fit(train_a[features].values, train_a[target])\n",
    "\n",
    "rf_pipe_highdepth_b = rf_pipe_highdepth.predict(train_b[features].values)\n",
    "rf_pipe_highdepth_c = rf_pipe_highdepth.predict(train_c[features].values)\n",
    "rf_pipe_highdepth_val = rf_pipe_highdepth.predict(train_val[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe.fit(train_a[features].values, train_a[target])\n",
    "\n",
    "knn_pipe_b = knn_pipe.predict(train_b[features].values)\n",
    "knn_pipe_c = knn_pipe.predict(train_c[features].values)\n",
    "knn_pipe_val = knn_pipe.predict(train_val[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc.fit(train_a[features].values, train_a[target])\n",
    "\n",
    "cbc_b = cbc.predict(train_b[features].values)\n",
    "cbc_c = cbc.predict(train_c[features].values)\n",
    "cbc_val = cbc.predict(train_val[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_b_meta   = np.c_[lr_preds_b, lgbm_lowdepth_b, lgbm_highdepth_b, cbc_b]\n",
    "part_c_meta   = np.c_[lr_preds_c, lgbm_lowdepth_c, lgbm_highdepth_c, cbc_c]\n",
    "part_val_meta = np.c_[lr_preds_val, lgbm_lowdepth_val, lgbm_highdepth_val, cbc_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple convex mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_bc_meta = np.concatenate((part_b_meta, part_c_meta), axis=0)\n",
    "y_bc_meta = pd.concat([train_b[target], train_c[target]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_to_try = np.linspace(0, 1, 1001)\n",
    "\n",
    "max_score = 0\n",
    "for alpha in alphas_to_try:\n",
    "    result = np.round(alpha*part_bc_meta[:, 0] + (1-alpha)*part_bc_meta[:, 1])\n",
    "    score = f1_score(y_bc_meta, result)\n",
    "\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        bs_alpha = alpha\n",
    "\n",
    "    \n",
    "best_alpha = bs_alpha\n",
    "r2_train_simple_mix = max_score\n",
    "\n",
    "print('Best alpha: %f; Corresponding 1 score on train: %f' % (best_alpha, r2_train_simple_mix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.round(best_alpha*part_val_meta[:, 1] + (1-best_alpha)*part_val_meta[:, 2])\n",
    "f1_simple_mix = f1_score(train_val[target], test_preds)\n",
    "\n",
    "print('Test F-1 for simple mix is %f' % f1_simple_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking for HoldOut Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8922902494331066"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack = LogisticRegression(C=0.001)\n",
    "\n",
    "stack.fit(part_b_meta, train_b[target])\n",
    "\n",
    "preds_meta_c = stack.predict(part_c_meta)\n",
    "f1_score(train_c[target], preds_meta_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.fit(part_bc_meta, y_bc_meta)\n",
    "val_meta_prediction = stack.predict(part_val_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8910835214446952"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(train_val[target], val_meta_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33515, 1184)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = pd.read_csv('../data/raw/data_test.zip', index_col='Unnamed: 0', low_memory=True)\n",
    "ids = ids[ids.x_exit.isnull()]\n",
    "\n",
    "data_test = test.merge(ids[['hash', 'trajectory_id']], on='hash')\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe.fit(train_train[features], train_train[target])\n",
    "\n",
    "rf_pipe_lowdepth.fit(train_train[features], train_train[target])\n",
    "rf_pipe_highdepth.fit(train_train[features], train_train[target])\n",
    "\n",
    "\n",
    "lgbm_lowdepth.fit(train_train[features], train_train[target])\n",
    "lgbm_highdepth.fit(train_train[features], train_train[target])\n",
    "\n",
    "cbc.fit(train_train[features], train_train[target])\n",
    "\n",
    "knn_pipe.fit(train_train[features], train_train[target])\n",
    "\n",
    "lr_sub_meta = lr_pipe.predict(data_test[features])\n",
    "\n",
    "rf_low_sub_meta = rf_pipe_lowdepth.predict(data_test[features])\n",
    "rf_high_sub_meta = rf_pipe_highdepth.predict(data_test[features])\n",
    "\n",
    "lgbm_low_sub_meta = lgbm_lowdepth.predict(data_test[features])\n",
    "lgbm_high_sub_meta  = lgbm_highdepth.predict(data_test[features])\n",
    "\n",
    "cbc_sub_meta = cbc.predict(data_test[features])\n",
    "\n",
    "knn_sub_meta = knn_pipe.predict(data_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_bc_meta = np.concatenate((part_b_meta, part_c_meta), axis=0)\n",
    "y_bc_meta = pd.concat([train_b[target], train_c[target]], axis=0)\n",
    "\n",
    "part_bcval_meta = np.concatenate((part_b_meta, part_c_meta, part_val_meta), axis=0)\n",
    "y_bcval_meta = pd.concat([train_b[target], train_c[target], train_val[target]], axis=0)\n",
    "\n",
    "stack.fit(part_bcval_meta, y_bcval_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_meta = np.c_[lr_sub_meta, lgbm_low_sub_meta, lgbm_high_sub_meta, cbc_sub_meta]\n",
    "yhat = stack.predict(sub_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    25110\n",
       "1.0     8405\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(yhat).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(list(zip(data_test['trajectory_id'], yhat)), columns=['id', 'target'])\n",
    "submission.to_csv('../data/submission_victor_stack5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking submission | local: 0.7460367786937224 | public: 0.8941"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test meta-features\n",
    "\n",
    "First, we will run models on train data and get predictions for test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7180858550316678"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipe.fit(train_train[features].values, train_train[target])\n",
    "lr_preds = lr_pipe.predict(train_val[features].values)\n",
    "\n",
    "f1_score(train_val[target], lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7493638676844784"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm.fit(train_train[features].values, train_train[target])\n",
    "lgbm_preds = lgbm.predict(train_val[features].values)\n",
    "\n",
    "f1_score(train_val[target], lgbm_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7058452812352294"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pipe.fit(train_train[features].values, train_train[target])\n",
    "rf_preds = rf_pipe.predict(train_val[features].values)\n",
    "\n",
    "f1_score(train_val[target], rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7463117382937781"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbc.fit(train_train[features].values, train_train[target])\n",
    "cbc_preds = cbc.predict(train_val[features].values)\n",
    "\n",
    "f1_score(train_val[target], cbc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta_features = np.c_[lr_preds, lgbm_preds, rf_preds, cbc_preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train meta-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_block = [12, 13, 14, 15]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
